\documentclass[a4paper,10pt]{book}
\usepackage[utf8]{inputenc}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{subfigure}
\usepackage{graphicx}
% \graphicspath{{./figures/}}
\usepackage{psfrag}
\usepackage[margin=1.5in]{geometry}


% \usepackage[pagewise]{lineno}
% \usepackage[notref,notcite]{showkeys}
\usepackage{color}
\def\red{\color{red}}
\def\blue{\color{blue}}
\newcommand{\conv}{\overline{\textsf{conv}}\,}
\newcommand{\pts}{\overline{\textsf{pts}}\,}
\newcommand{\adm}{{\textsf{ADM}}}
\newcommand{\tw}{{\textsf{TW}\,}}
\newcommand{\mass}{{\textsf{mass}\,}}
\newcommand{\J}{{\textsf{J}}}
\newcommand{\p}{{\textsf{p}}}
\newcommand{\const}{{\textsf{Const.}}}
\renewcommand{\H}{{\mathcal{H}}}
\renewcommand{\P}{{\mathcal{P}}}

\newcommand{\res}{{\llcorner}}

\def\ii{{\textrm{int}}\,}
\def\cl{{\textrm{cl}}\,}
\def\ran{{\textrm{Ran}}\,}
\def\ker{{\textrm{Ker}}\,}
\def\rank{{\textrm{rank}}\,}
\def\sign{{\textrm{sign}}\,}

\def\eps{{\varepsilon}}
\def\R{{\mathbb{R}}}
\newcommand{\spn}{{\textsf{span}}\,}
\renewcommand{\S}{{\mathcal{S}}}
\renewcommand{\dim}{{\textrm{dim}}\,}
\renewcommand{\l}{{\big\langle}}
\renewcommand{\r}{{\big\rangle}}

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{thm1}{Theorem}[section]
\newtheorem{defn}[thm1]{Definition}
\newtheorem{proposition}[thm1]{Proposition}
\newtheorem{corollary}[thm1]{Corollary}
\newtheorem{thm4}[thm1]{Observation}
\newtheorem{lemma}[thm1]{Lemma}
\numberwithin{equation}{section}

\theoremstyle{remark}
\newtheorem{remark}[thm1]{Remark}

\newcommand{\lecturetitle}[1]{%
  \vspace{1cm} % Add vertical space before the title
  \begin{center}
    \Huge \textbf{#1} % Large, bold title
  \end{center}
  \vspace{0.5cm} % Add vertical space after the title
  \hrulefill % Add a horizontal line
  \vspace{0.5cm}
}


\begin{document}

\chapter{What is Numerical Analysis about ?}


{\Large \textbf{Case 1. Minimizer}}

\bigskip

\begin{theorem}
 Suppose $f:[a,b] \rightarrow \R$ is continuous on $[a,b]$. Let $m:=\inf \{f(x) ~|~ x\in[a,b]\}$. Then there {{exists}} a minimizer $x_*\in[a,b]$ such that $f(x_*) = m$.
\end{theorem}
Note: we have used that a continuous function $f$ on $[a,b]$ is bounded.

\begin{proof}
 Step 1. For each $n=1,2,3,\cdots$ let $x_n \in [a,b]$ be any element such that $$f(x_n) < m + \frac{1}{n}.$$
 For each $n$, such an element must exist: Otherwise, i.e., if $f(x) \ge m + \frac{1}{n}$ for every $x\in [a,b]$, then $m+ \frac{1}{n}$ is a lower bound of the range set that is strictly bigger than $m$, contradicting to the definition of $m$.
 
 Step 2. We have defined the sequence $(x_n)$, and let $y_n:=f(x_n)$. Then $y_n \rightarrow m$ as $n \rightarrow \infty$ because
 $$0\le|y_n-m| < \frac{1}{n}.$$
 
 Step 3. $(x_n)$ is a sequence of real numbers in $[a,b]$, and thus $(x_n)$ is a bounded sequence. Therefore, there exists a convergent subsequence $(x_{n_k})_{k=1}^\infty$ to a limit $x_*$. Furthermore, because $[a,b]$ is closed, the limit point $x_*\in [a,b]$. We also know that the subsequence $(y_{n_k})$ of a convergent sequence $(y_n)$ also is convergent to the same limit $m$.
 
 Step 4. Now, we observe
 $$ m = \lim_{k \rightarrow \infty} y_{n_k} = \lim_{k \rightarrow \infty} f(x_{n_k}) = f(x_*),$$
 where in the last equality, we used the fact that $f$ is continuous on $[a,b]$.
\end{proof}

\newpage

{\Large \textbf{Case 2 : Riemann Integral or Quadrature}}

\bigskip

Let us consider high-school series calculations. Let $n$ be a fixed natural number. For each $i$, we let $x_i = a + i \frac{b-a}{n}$. We let $t_{i}$ be any point belogs to $[x_{i-1}, x_i]$ for $i=1,\cdots n$. 

{The definition} of the Riemann Integral of $f$ over $[a,b]$ is justified by the existence of a real number $I$ such that 
 \begin{align*}
& \lim_{n \rightarrow \infty} \sum_{i=1}^n f(t_{i}) ~\frac{b-a}{n} = I.
 \end{align*}
regardless of choice $(t_{i})$ at each $n$. 
We examine the proof of the assertion.

\begin{proof}
 Step 1. By extreme value theorem on each compact interval $[x_{i-1},x_i]$, there exist the minimizer $t_{i*}$ and $t_{i}^*$ in $[x_{i-1},x_i]$. Then we have for each $n$
 \begin{align*}
  \sum_{i=1}^n f(t_{i*}) ~\frac{b-a}{n} \le \sum_{i=1}^n f(t_{i}) ~\frac{b-a}{n} \le \sum_{i=1}^n f(t_{i}^*) ~\frac{b-a}{n}.
 \end{align*}
 
 Step 2. We omit the proof that $\Big(\sum_{i=1}^n f(t_{i*}) ~\frac{b-a}{n}\Big)_{n=1}^\infty$ and $\Big(\sum_{i=1}^n f(t_{i}^*) ~\frac{b-a}{n}\Big)_{n=1}^\infty$ are bounded monotone sequences so that their respective limits $\alpha,\beta \in \R$, $\alpha\le \beta$ exist.

 Step 3. That $f$ is continuous in $[a,b]$ implies the following: 
 \begin{align*}
  &\text{For every $\epsilon>0$, there exists $\delta>0$ such that}\\
  & x,x' \in [a,b], |x-x'| < \delta  \quad \Longrightarrow \quad |f(x)-f(x')| < \frac{\epsilon}{b-a}.
 \end{align*}
that is to say, $f$ is uniformly continuous in $[a,b]$. For a given $\epsilon$, we take $K\in\mathbb{N}$ so that $\frac{b-a}{K}<\delta$ for each $\delta$. This gives that
\begin{align*}
 &\text{for every $\epsilon$, there exists $K$ such that for $n\ge K$},\\
 &\Big|\sum_{i=1}^n f(t_{i}^*) ~\frac{b-a}{n} - \sum_{i=1}^n f(t_{i_*}) ~\frac{b-a}{n}\Big| \le \sum_{i=1}^n \Big|f(t_{i}^*)-f(t_{i*})\Big| ~\frac{b-a}{n} \le \sum_{i=1}^n \frac{\epsilon}{b-a} ~\frac{b-a}{n} = \epsilon.
\end{align*}
This implies that the limits $\alpha=\beta$. By the Squeeze theorem, $\Big(\sum_{i=1}^n f(t_{i}) ~\frac{b-a}{n}\Big)_{n=1}^\infty$ is convergent, to the same limit. The limit is defiend to be $I$.
\end{proof}

\bigskip

\bigskip

We will use quite much better working numerical methods (midpoint rule, simpson rule, gaussian quadrature, $\cdots$) than the one suggested in the proof.

\newpage

{\Large \textbf{Case 3 : rank of a matrix}}

\bigskip

Let $A$ be an $n \times k$ real matrix of $\mathbb{R}^{n\times k}$. That the very first fundamental notion for $A$ is the rank of $A$, the number being {defined} by the fact that
$$ \text{the number of linearly independent columns} = \text{the number of linearly independent rows}.$$
Denote the $(LHS) = r_{col}(A)$ and $(RHS) = r_{row}(A)$. This fundamental equality is a consequence of the inequality that $$\text{for any matrix $A$,} \quad r_{row}(A) \le r_{col}(A)$$
because if that is true then $ \Big(r_{col}(A) =\Big) r_{row}(A^T) \le r_{col}(A^T) \Big(= r_{row}(A)\Big)$.

The inequality follows from the assertion 
  \begin{align*}
  &\text{if $r_{i_1}, r_{i_2}, \cdots r_{i_\rho}$ are linearly independent rows of $A$}\\
  &\text{then $Ar_{i_1}, Ar_{i_2}, \cdots Ar_{i_\rho}$ are linearly independent vectors in $\R^n$.}
 \end{align*}
Here we denote the rows of $A$ from top to bottom by $r_1$, $r_2$, $\cdots$, $r_n$. 
We examine the proof of the assertion.

\begin{proof}

 Suppose not, i.e., we have $(\lambda_1, \lambda_2, \cdots, \lambda_\rho) \ne 0$ such that $\sum_{\alpha=1}^\rho \lambda_\alpha \big(Ar_{i_\alpha}\big)=0$. Then,
 \begin{align*}
  0 = \sum_{i=\alpha}^\rho \lambda_\alpha (Ar_{i_\alpha})=\sum_{\alpha=1}^\rho  A(\lambda_\alpha r_{i_\alpha}) = A \Big(\sum_{\alpha=1}^\rho \lambda_\alpha r_{i_\alpha}\Big).
 \end{align*}
 But,  \begin{align*}
  Ax = 0 \quad &\Longleftrightarrow \quad x \perp \spn\l r_1, r_2, \cdots, r_n \r.
 \end{align*}
 Combining the two observations, $$\sum_{\alpha=1}^\rho  \lambda_\alpha r_{i_\alpha} \in \spn\l r_1, r_2, \cdots, r_n \r \cap \spn\l r_1, r_2, \cdots, r_n \r^\perp = \{0\}.$$
 Thus, we conclude that $\sum_{\alpha=1}^\rho  \lambda_\alpha r_{i_\alpha}$ is a zero vector. Recalling that $r_{i_1}, r_{i_2}, \cdots r_{i_\rho}$ are linearly independent, we conclude $\lambda_1=\lambda_2=\cdots=\lambda_\rho = 0.$ Contradiction.
\end{proof}

\bigskip

\bigskip

We have to wait until we prove that any $A$ admits a Singular Value Decomposition to {\textsf{compute}} the rank of $A$.

\newpage

{\Large \textbf{Case 4} : Jordan factorization of a square matrix $T$.}

\bigskip

Another striking example where we do not have an algorithm for it is the Jordan factorization.

\bigskip

\bigskip

We will not prove the statement here, that any square matrix $T$ admits a Jordan factorization $T=PJP^{-1}$. But you will see in its proof that it does not provide a constructive method. There is a fundamental obstacle that prevents us to compute it in general case. 

\newpage

Any pair of (theory,method) from variety of mathematical areas.

\bigskip 

What will we treat in this course ?


\begin{enumerate}
 \item Existence of interpolating / approximating polynomials for a class of functions. Topics: spline, Tchebyshev polynomials, $\cdots$. 1d, multi-d.
 \item Existence of fixed point and Newton's method on nonlinear system.
 \item Existence of solution of initial value problem of nonlinear ode / Runge-Kutta methods, $\cdots$
 \item For all of above, error analysis methodology.
 \item (2nd semester) Linear algebra theory
 \item (2nd semester) more on (partial) differential equations.
\end{enumerate}

\chapter{How does my computer throw $\sin(x)$ for an input $x$?}

{\Large \textbf{Case 1. A function from $\{1,2,3,4,5,6,7,8,9,10\}$ to $\R$}}

\bigskip

We can define a function $a: \{1,2,3,4,5,6,7,8,9,10\} \mapsto \R$, using the table.

\begin{table}[htbp]
\centering
 \begin{tabular}{c| c| c| c| c| c| c| c| c| c| c}
  $n$& 1&2&3&4&5&6&7&8&9&10\\
  \hline
  $a_n$ & &&&&&&&&&
 \end{tabular}
\end{table}

\noindent{\Large \textbf{Case 2. A function from $\mathbb{N}$ to $\R$}}

\bigskip
\bigskip 

Are there ways to define a function from countably infinite set to $\R$?

\bigskip
\bigskip 

\noindent{\Large \textbf{Case 3. A function from $[0,1]\subset\R$ to $\R$}}

\bigskip
\bigskip 

Are there ways to define a function from this uncountably infinite set $[0,1]$ to $\R$?

\bigskip
A function on $E \subset \R^n$ ?

\bigskip
\bigskip 

\noindent\hrulefill

\bigskip
\bigskip

A function $f: X \mapsto Y$ is a correspondence $G \subset X\times Y$ satisfying the rule that 
$$\text{For every $x\in X$, there exists unique $y\in Y$ so that $(x,y)\in G$.}$$

Is a function a black box ?

Is a function a table ?

Can you come up with an example of a function that you can build as a machine ?

How does my computer know which value to return for $\sin(x)$?

\newpage

I wanted you to see the importance of a class of functions on $\R^n$ that are
\begin{enumerate}
 \item piecewise polynomial, and
 \item each piece's domain of definition (the support, more precisely,) is elementary in the sense that you can complete the membership check of a given $x$ to the domain in a finite procedures.
\end{enumerate}

A continuous function (in undergraduate courses) and a Borel measurable function (in graduate courses) can be defined by suitable limits (uniform limit and pointwise limit respectively) of such pp functions.

\bigskip
\bigskip

\noindent\hrulefill

\bigskip
\bigskip 

\noindent{\Large \textbf{(Local) polynomial interpolation}}

\bigskip
\bigskip 


Let $I=[a,b]$. We consider
\begin{align*} 
C^{n+1}([a,b]) = \big\{ f:[a,b] \rightarrow \R ~|~ &\text{$f$ is $n+1$ times differentiable and }\\
&\text{$f$, $f'$, $f''$, $\cdots$, $f^{(n+1)}$ are continuous in $[a,b]$}\big\}. 
\end{align*}
Note: At the endpoint $a$ and $b$, the differentibility is the right-differentiability and the left-differentiability respectively.

\bigskip

We consider the following interpolation problem:
\begin{enumerate}
 \item Suppose we sample the data $(x_0,f(x_0))$, $\cdots$, $(x_n,f(x_n))$ for $f \in C^{n+1}([a,b])$ and $x_0,\cdots,x_n \in [a,b]$.
 \item We look for a polynomial $p$ on $[a,b]$ with
 $$ p(x_i) = f(x_i) \quad i=0,1,2,\cdots n.$$
 \item We seek a formula of the remainder $R(x)$,
 $$R(x) = f(x) - p(x), \quad x\in[a,b]$$
\end{enumerate}

In fact, the term {\it interpolation} does not necessarily assume the function $f$ and does not assume that the data are collected by sampling. Nevertheless, it is convenient to speak of the function $f$.

We are going to prove the far-reaching remainder theorem, which is much stronger than the Taylor's theorem. We recall the Taylor's theorem for the comparison.

\begin{theorem*}[Taylor's theorem]
 Let $f\in C^{n+1}([a,b])$ and $x_0 \in [a,b]$. Then for $x\in[a,b]$,
 \begin{align*}
 R(x) &= f(x) - \Big( f(x_0) + f'(x_0) (x-x_0) + \frac{f''(x_0)}{2!} (x-x_0)^2 + \cdots + \frac{f^{(n)}(x_0)}{n!} (x-x_0)^n \Big) \\ 
 &= \int_{\tau_1=0}^1 \int_{\tau_2=0}^{1-\tau_1} \int_{\tau_3=0}^{1-\tau_1-\tau_2} \cdots \int_{\tau_n=0}^{1-\tau_1-\cdots-\tau_{n-1}} \int_{\tau_{n+1}=0}^{1-\tau_1-\cdots-\tau_{n}}\\
 & \frac{d^kf}{dx^k} \Big( (1-\tau_1-\tau_2-\cdots-\tau_n-\tau_{n+1})x + (\tau_1+\tau_2+\cdots+\tau_n+\tau_{n+1})x_0 \Big) \: d\tau_{n+1} d\tau_{n}\cdots d\tau_1\\
 & \times (x-x_0)^{n+1}\\
 &= \frac{1}{(n+1)!} \frac{d^{n+1}f}{dx}(\xi) (x-x_0)^{n+1} \quad \text{for some $\xi \in [a,b]$.}
 \end{align*}
 \end{theorem*}

\chapter{Divided differences and\\ 1d Remainder Theorem}

Let $I=[a,b]$, $n\in \mathbb{N}$, and consider $k+1$  points $(k \in \{0,1,2,\cdots, n\})$
$$x_0,x_1,x_2,\cdots,x_k\in [a,b] \quad \text{(they may be repeated)}.$$ We will denote the choice of $(k+1)$ points by 
$$[x_0,x_1,\cdots,x_k].$$

A choice $[x_0,\cdots,x_k]$ acts on the class $C^{n+1}([a,b])$: For $f \in C^{n+1}([a,b])$,
\begin{align*}
 \text{if $k=0$}& &\big([x_0], f\big) &=f(x_0),\\
 \text{if $k\ge 1$}& &\big([x_0,x_1,\cdots,x_k],f\big) &:= \frac{1}{\sqrt{k+1}}\int_{\Lambda^k \subset \R^{k+1}} \frac{d^kf}{dx^k} \big( \lambda_0x_0 + \cdots \lambda_k x_k\big) \: dS (\lambda).
\end{align*}
\begin{align*}
\text{The set $\Lambda^k$}&= \big\{ (\lambda_0,\lambda_1,\cdots,\lambda_k) \in \R^{k+1} ~|~ \forall e ~~ \lambda_e\ge 0, \quad \sum_{e=0}^{k} \lambda_e = 1\big\} \subset \R^{k+1}.
 \end{align*}
The integral is denoted by $\big([x_0,x_1,\cdots,x_k],f\big)$, or $[x_0,\cdots,x_k]f$, or $f[x_0,x_1,\cdots,x_k]$.

\newpage

\noindent{\Large \textbf{1d remainder theorem}}

\bigskip
\bigskip

The objective is to prove the following 1d remainder theorem. Having defined the divided differences for $f\in C^{n+1}([a,b])$, we can state the far-reaching remainder theorem.

\begin{theorem*}[1d remainder theorem] Let $f\in C^{n+1}([a,b])$ and $x_0$, $x_1$, $\cdots$, $x_n$ $\in [a,b]$. Then, for $x \in[a,b]$,
\begin{align*}
 R(x) &= f(x) - \Big(f[x_0] + f[x_0,x_1](x-x_0) + f[x_0,x_1,x_2](x-x_0)(x-x_1) + \cdots \\
 &+ f[x_0,x_1,\cdots,x_n](x-x_0)(x-x_1)(x-x_2)\cdots(x-x_{n-1}) \Big)\\
 &= f[x,x_0,x_1,x_2,\cdots,x_n] (x-x_0)(x-x_1)\cdots(x-x_{n-1})(x-x_n).
\end{align*}
\end{theorem*}
\begin{remark}
 Taylor's theorem is a special case where $x_0=x_1=\cdots=x_n = \bar{x}$.
\end{remark}

\newpage

\noindent{\Large \textbf{Properties of $f[x_0,x_1,\cdots,x_k]$}}

\bigskip
\bigskip
\begin{enumerate}
 \item[(1)] The integral is independent of orders in the $[x_0,x_1,\cdots,x_k]$.
 \item[(2)] $k=1$ cases:
 \begin{align*}
  &\text{if $x_0\ne x_1$,} & f[x_0,x_1] &= \frac{f(x_1)-f(x_0)}{x_1-x_0} = \frac{f(x_0)-f(x_1)}{x_0-x_1},\\
  &\text{if $x_0=x_1= \bar{x}$,}& f[\bar{x},\bar{x}] &= f'(\bar{x}).
 \end{align*}
 \item[(3)] For $k\ge1$, the equality always holds:
 \begin{align*}
  f[x_0,x_1,\cdots,x_k] ~ (x_k - x_0) = f[x_1,x_2,\cdots,x_k] - f[x_0,x_1,\cdots,x_{k-1}].
 \end{align*}
 Indeed, above two $k=1$ cases lead to that
 $$ f[x_0,x_1] (x_1-x_0) = f[x_1] - f[x_0].$$
 \item[(4)] No matter how points are repeated partially, the integral is always well-defined for $f\in C^{n+1}{[a,b]}$. The very special case $x_0=x_1=\cdots=x_k = \bar{x}$ leads to that
 $$f[\bar{x},\bar{x},\cdots,\bar{x}] = \frac{1}{k!} \frac{d^kf}{dx^k}(\bar{x}) \quad ~~ (\text{$k+1$ times repeated})~~$$

\end{enumerate}

\newpage

\noindent{\Large \textbf{Proof of 1d remainder theorem}}

\bigskip
\bigskip


We recall the equality
  \begin{align*}
  f[x_0,x_1,\cdots,x_k] ~ (x_k - x_0) = f[x_1,x_2,\cdots,x_k] - f[x_0,x_1,\cdots,x_{k-1}].
 \end{align*}

For $k\in\{0,1,2,\cdots,n\}$, we let
 \begin{align*}
 p_k(x) &= f[x_0] + f[x_0,x_1](x-x_0) + f[x_0,x_1,x_2](x-x_0)(x-x_1) + \cdots \\ 
 &+ f[x_0,x_1,\cdots,x_k](x-x_0)(x-x_1)(x-x_2)\cdots(x-x_{k-1}).
 \end{align*}

 
 \begin{proof}[proof of 1d remainder theorem]
 We assert that for $k\in\{0,1,2,\cdots,n\}$,
 $$ f(x) - p_k(x) = f[x,x_0,x_1,\cdots,x_k] (x-x_0)(x-x_1)(x-x_2)\cdots(x-x_{k}).$$
 At $k=0$, by the equality
 $$ f(x) - p_0(x) = f(x)- f(x_0) = f[x,x_0] (x-x_0).$$
 Now, if the assertion is true for $0,1,2,\cdots,k-1$,
 \begin{align*}
 f(x) - p_k(x) &= f(x) - p_{k-1}(x) - f[x_0,x_1,\cdots,x_k](x-x_0)(x-x_1)(x-x_2)\cdots(x-x_{k-1})\\
 &= f[x,x_0,x_1,\cdots,x_{k-1}](x-x_0)(x-x_1)(x-x_2)\cdots(x-x_{k-1})\\
 &- f[x_0,x_1,\cdots,x_k](x-x_0)(x-x_1)(x-x_2)\cdots(x-x_{k-1})\\
%  &= \Big(f[x,x_0,x_1,\cdots,x_{k-1}]- f[x_0,x_1,\cdots,x_k]\Big)\Big((x-x_0)(x-x_1)(x-x_2)\cdots(x-x_{k-1})\Big)\\
 &= f[x,x_0,x_1,\cdots,x_{k-1},x_k]\Big((x-x_0)(x-x_1)(x-x_2)\cdots(x-x_{k-1})(x-x_k)\Big).
 \end{align*}
\end{proof}

\newpage

\noindent{\Large \textbf{Change of variables formula for the integral}}

\bigskip
\bigskip

We can check those properties, using the change of variable formula: 

\bigskip

For $k\ge 1$,
$$ \text{The set $\tau^k$} =\big\{ (\tau_1,\tau_2,\cdots,\tau_k) \in \R^k ~|~ 0 \le \tau_1 + \cdots + \tau_k\le 1\big\}. $$

For $k\ge 1$,

\begin{align*}
 &\frac{1}{\sqrt{k+1}}\int_{\Lambda^k \subset \R^{k+1}} \frac{d^kf}{dx^k} \big( \lambda_0x_0 + \cdots +\lambda_k x_k\big) \: dS (\lambda)\\
 (\text{change of variable}) \quad=& \int_{\tau^k \subset \R^k} \frac{d^kf}{dx^k} \big( (1-\tau_1-\tau_2-\cdots-\tau_k)x_0 + \tau_1x_1 + \cdots +\tau_k x_k\big) \: d\mathcal{L}^k (\tau)\\
 (\text{Fubini}) \quad =& \int_{\tau_1=0}^1 \int_{\tau_2=0}^{1-\tau_1} \int_{\tau_3=0}^{1-\tau_1-\tau_2} \cdots \int_{\tau_k=0}^{1-\tau_1-\cdots-\tau_{k-1}} \\
 &\frac{d^kf}{dx^k} \big( (1-\tau_1-\tau_2-\cdots-\tau_k)x_0 + \tau_1x_1\cdots \tau_k x_k\big) \: d\tau_k d\tau_{k-1}\cdots d\tau_1
\end{align*}
In particular,
\begin{align*}
 &\frac{1}{\sqrt{k+1}}\int_{\Lambda^k \subset \R^{k+1}} 1 \: dS (\lambda) \\
 =& \int_{\tau_1=0}^1 \int_{\tau_2=0}^{1-\tau_1} \int_{\tau_3=0}^{1-\tau_1-\tau_2} \cdots \int_{\tau_k=0}^{1-\tau_1-\cdots-\tau_{k-1}} 1 \: d\tau_k d\tau_{k-1}\cdots d\tau_1\\
 =& \frac{1}{k!}, \quad \text{the volume of $k$-simplex $\tau^k$}.
\end{align*}


\newpage

\noindent{\Large \textbf{Mean Value of Integrand is attained}}

\bigskip
\bigskip

Although we will not use this much later, it is worth to know that the mean value of the integrand is attained. 
\begin{theorem*} Let $f\in C^{n+1}([a,b])$, $k \in \{0,1,2,\cdots, n\}$, and $x_0,x_1,x_2,\cdots,x_k\in [a,b]$. Then there exists $\xi\in[a,b]$ such that
\begin{align*}
f[x_0,x_1,\cdots,x_k] = \frac{1}{k!}\frac{d^kf}{dx^k}(\xi).
\end{align*}
\end{theorem*}
\begin{proof}
For $\lambda \in \Lambda^k$, we denote the linear function
$$x(\lambda) = \lambda_0x_0 + \lambda_1 x_1 + \cdots + \lambda_k x_k \in [a,b].$$

 Since $$\Lambda^k \ni (\lambda_0,\lambda_1,\cdots,\lambda_k) \mapsto \frac{d^kf}{dx^k} (\lambda_0 x_0 + \cdots + \lambda_k x_k) = \frac{d^kf}{dx^k}\circ x (\lambda)$$
is a continuous function on the compact set $\Lambda^k$, there exist $\lambda^*$, $\lambda_* \in \Lambda^k$, the maximizer and the minimizer. 

If $\lambda_* = \lambda^*$, then $\frac{d^kf}{dx^k}\circ x$ must be a constant function over $\Lambda^k$, that is 
$$\frac{d^kf}{dx^k} (\xi) \quad \text{with $\xi = x(\bar\lambda)$ for any $\bar\lambda\in \Lambda^k$}$$ 
and thus the integral
$$ \frac{1}{\sqrt{k+1}}  \frac{d^kf}{dx^k} (\xi)\int_{\Lambda^k} 1 \: dS(\lambda) = \frac{1}{k!}\frac{d^kf}{dx^k} (\xi).$$
We are done for this case.

Now, we assume $\lambda_*\ne \lambda^*$. Note that the convex set $\Lambda^k$ contains the line segment joining $\lambda_*$ and $\lambda^*$. Therefore, we can define a function 

$$ h(t): [0,1] \ni t \mapsto \frac{d^kf}{dx^k} \circ x\big((1-t)\lambda_* + t\lambda^*\big).$$
$h$ is a continuous function on $[0,1]$. By intermediate value theorem, every values in the range $[h(0),h(1)]=\big[\frac{d^kf}{dx^k}(\lambda_*), \frac{d^kf}{dx^k}(\lambda^*)]$ is attained at some $t \in [0,1]$. Certainly,

\begin{align*}
 &h(0)=\frac{d^kf}{dx^k}\big(x(\lambda_*)\big) \le \frac{d^kf}{dx^k}\big(x(\lambda)\big) \le \frac{d^kf}{dx^k}\big(x(\lambda^*)\big)=h(1). \quad \text{This implies}\\
&\frac{k!~\frac{d^kf}{dx^k}\big( x(\lambda_*)\big)}{\sqrt{k+1}}\int_{\Lambda^k \subset \R^{k+1}}  1 \: dS (\lambda) \le
\frac{k!}{\sqrt{k+1}}\int_{\Lambda^k \subset \R^{k+1}} \frac{d^kf}{dx^k} \big( x(\lambda)\big) \: dS (\lambda) \le
\frac{k!~\frac{d^kf}{dx^k} \big( x(\lambda^*)\big)}{\sqrt{k+1}}\int_{\Lambda^k \subset \R^{k+1}} 1 \: dS (\lambda), 
\end{align*}
but note the leftmost term is $h(0)$ and the rightmost term is $h(1)$.
Therefore, for some $\bar{t}\in [0,1]$ 
\begin{align*}
 \frac{k!}{\sqrt{k+1}}\int_{\Lambda^k \subset \R^{k+1}} \frac{d^kf}{dx^k} \big( x(\lambda)\big) \: dS (\lambda) = h(\bar{t}).
\end{align*}
Now, $h(\bar{t}) = \frac{d^kf}{dx^k}(\xi)$ if $\bar\lambda = (1-\bar t) \lambda_* + \bar t \lambda^*$, and $\xi = x(\bar\lambda)$.
\end{proof}

\chapter{(Local) Polynomial Interpolation}


\noindent{\Large \textbf{Case 1. Data $\big(x_0,f(x_0)\big), \cdots, \big(x_n,f(x_n)\big)$, $x_i$ all distinct.}}

\bigskip
\bigskip

We definde $\P^{<n+1}([a,b]) \subset C^{n+1}([a,b])$ be the set of all polynomials of order less than $n+1$, tautologically, the set of all polynomials of order at most $n$.
We define the condition
\begin{align}\label{A} \tag{$A$}
 &x_0, x_1, \cdots, x_n \in [a,b] \quad \text{and they are all distinct}.
\end{align}
\begin{theorem}[existence] For any $f \in C^{n+1}([a,b])$ and $x_0,x_1,\cdots,x_n$ satisfying \eqref{A}, there exists a polynomial $p\in \P^{<n+1}([a,b])$ such that
$$ p(x_i) = f(x_i) \quad \text{for every $i=0,1,2,\cdots,n$.}$$
\end{theorem}
\begin{proof}
It suffices to recall that 
 \begin{align*}
f(x) - p_n(x) = f[x,x_0,x_1,\cdots, x_n](x-x_0)(x-x_1)\cdots(x-x_n),
 \end{align*}
and $p_n \in \P^{<n+1}([a,b])$. $f[x,x_0,x_1,\cdots, x_n]$ is bounded in $[a,b]$ because $f\in C^{n+1}([a,b])$. The conclusion follows by plugging in $x_i$ in place of $x$.
\end{proof}


\begin{theorem}[uniqueness] Under the same assumptions, the interpolating polynomial in $\P^{<n+1}([a,b])$ is unique.
\end{theorem}
\begin{proof}
 Suppose $p, q \in \P^{<n+1}([a,b])$ be the two interpolating polynomials. Then $r=p-q \in \P^{<n+1}([a,b])$ too and
 $$r(x_i) = 0 \quad \text{for every $i=0,1,2,\cdots,n$.}$$
 Since $x_0$,$\cdots$, $x_{n-1}$ are roots of $r$,
 $$ r(x)=A(x-x_0)(x-x_1)\cdots(x-x_{n-1}). \quad \text{But}$$
 $$r(x_n) = A(x_n-x_0)(x_n-x_1)\cdots(x_n-x_{n-1}) = 0.$$
 Since $x_0, x_1, \cdots, x_n$ are all distinct, we conclude $A=0$, or $p=q$.
\end{proof}

\newpage

\noindent{\Large \textbf{Case 2. Data $\big(x_1,f(x_1)\big), \cdots, \big(x_n,f(x_n)\big)$,\\ $\big(x_1,f'(x_1)\big), \cdots, \big(x_n,f'(x_n)\big)$, $x_i$ all distinct.}}

\bigskip
\bigskip
\begin{theorem}[existence] For any $f \in C^{2n}([a,b])$ and $x_1,\cdots,x_n$ satisfying \eqref{A}, there exists a polynomial $p\in \P^{<2n}([a,b])$ such that
$$ p(x_i) = f(x_i), \quad p'(x_i) = f'(x_i)\quad \text{for every $i=1,2,\cdots,n$.}$$
\end{theorem}
\begin{proof}
We consider a choice of $2n$ points in $[a,b]$ that is
$$[x_1,x_1,x_2,x_2,x_3,x_3,\cdots, x_n,x_n].$$
It suffices to recall that 
 \begin{align*}
f(x) - p_{2n}(x) = f[x,x_1,x_1,x_2,x_2,\cdots, x_n,x_n](x-x_1)^2(x-x_2)^2\cdots(x-x_n)^2=R(x),
 \end{align*}
where
\begin{align*}
p_{2n}(x) &= f[x_1] + f[x_1,x_1](x-x_1) + f[x_1,x_1,x_2](x-x_1)^2 +  f[x_1,x_1,x_2,x_2](x-x_1)^2(x-x_2) + \cdots\\
&+ f[x_1,x_1,x_2,x_2,x_3,x_3,\cdots,x_n,x_n](x-x_1)^2(x-x_2)^2\cdots(x-x_{n-1})^2(x-x_n)
\end{align*}
in $\P^{<2n}([a,b])$. {\red $f[x,x_1,x_1,\cdots, x_n,x_n]$ is $C^1[a,b]$ because $f\in C^{2n}([a,b])$.} The conclusion follows by plugging in $x_i$ in place of $x$, for $R(x)$ and the derivative $R'(x)$.
\end{proof}


\begin{theorem}[uniqueness] Under the same assumptions, the interpolating polynomial in $\P^{<2n}([a,b])$ is unique.
\end{theorem}
\begin{proof}
 Suppose $p, q \in \P^{<n+1}([a,b])$ be the two interpolating polynomials. Then $r=p-q \in \P^{<2n}([a,b])$ too and
$x_i$, $i=1,2,3,\cdots,n$ are all double roots of $r$. We write
 $$ r(x)=A(x-x_1)^2(x-x_2)^2\cdots(x-x_{n-1})^2(x-x_n). \quad \text{But}$$
 $$r'(x_n) = A(x_n-x_1)^2(x_n-x_1)^2\cdots(x_n-x_{n-1})^2 = 0.$$
 Since $x_1, x_2, \cdots, x_n$ are all distinct, we conclude $A=0$, or $p=q$.
\end{proof}

\bigskip
\noindent\hrulefill
\bigskip

\begin{remark}
 Note that we can in general achieve interpolation with a choice 
 $$[x_0,x_1,x_2,\cdots,x_r]$$
 in $\P^{<r+1}([a,b])$, allowing points to be repeated in any certain way. Taylor polynomial is one such a special case.
\end{remark}























\end{document}
